{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 NSGA II\n",
    "\n",
    "> Code is inspired from:\n",
    "> \n",
    "> https://medium.com/@rossleecooloh/optimization-algorithm-nsga-ii-and-python-package-deap-fca0be6b2ffc\n",
    ">\n",
    "> https://github.com/DEAP/deap/blob/master/examples/ga/nsga2.py\n",
    ">\n",
    ">  https://github.com/DEAP/deap/blob/master/deap/tools/emo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from distutils.command.build_scripts import first_line_re\n",
    "from tkinter.tix import COLUMN\n",
    "# Import deque for the stack structure, copy for deep copy nodes\n",
    "from collections import deque\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import (DecisionTreeClassifier, DecisionTreeRegressor,\n",
    "                          ExtraTreeClassifier)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "# Encoding categorical features with preserving the missing values in incomplete features\n",
    "from sklearn.preprocessing import (KBinsDiscretizer, LabelEncoder,\n",
    "                                   OneHotEncoder, OrdinalEncoder,\n",
    "                                   StandardScaler)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import array\n",
    "import random\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import benchmarks\n",
    "from deap.benchmarks.tools import diversity, convergence, hypervolume\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define some constants for the genetic algorithm\n",
    "CONSTANTS_DICT = {\n",
    "    \"POPULATION_SIZE\": 100, # number of individuals in each population\n",
    "    \"MAX_GENERATIONS\": 200, # number of generations to run the algorithm\n",
    "    \"CROSSOVER_RATE\": 1.0, # crossover rate should always be 100%, based on slides\n",
    "    \"MUTATION_RATE\": 0.2, # mutation rate\n",
    "    \"CLASSIFIER\": KNeighborsClassifier(), # classifier to use\n",
    "    \"BOUND_LOW\": 0.0, # lower bound for the features\n",
    "    \"BOUND_UP\": 1.0, # upper bound for the features\n",
    "    \"ETA\": 20.0, # crowding degree for mutation  and crossover\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Empty\n",
    "\n",
    "\n",
    "class DatasetPart3:\n",
    "    def __init__(self, df) :\n",
    "        self.df=df\n",
    "        self.df.columns = self.df.columns.str.strip()\n",
    "        self.x = self.df.iloc[:,:-1]\n",
    "        self.y = self.df.iloc[:,-1]\n",
    "        self.M = self.df.shape[0]  # number of rows\n",
    "    \n",
    "    @classmethod\n",
    "    def constructFromFile(cls, filePath):\n",
    "        \"\"\"Depends on different ds\"\"\"\n",
    "        pass\n",
    "\n",
    "    def getDfWithSelectedFeatures(self, selectedFeatures:list):\n",
    "        \"\"\"No need to avoid FS bias, just based on df\"\"\"\n",
    "        returnedDf = pd.DataFrame()\n",
    "        selectedCount = 0\n",
    "        for i in range(len(selectedFeatures)):\n",
    "            isSelected = True if selectedFeatures[i] == 1 else False\n",
    "            if isSelected:\n",
    "                selectedCount += 1\n",
    "                # concat this feature to the returned dataframe\n",
    "                returnedDf = pd.concat([returnedDf,self.df.iloc[:,i]],axis=1)\n",
    "        # concat the class column\n",
    "        returnedDf = pd.concat([returnedDf, self.df.iloc[:,-1]],axis=1)\n",
    "        return returnedDf, selectedCount\n",
    "    \n",
    "    @staticmethod\n",
    "    def run_model(df:pd.DataFrame, classifier = CONSTANTS_DICT[\"CLASSIFIER\"]):\n",
    "        # pipe = Pipeline([\n",
    "        #     ('scaler', StandardScaler()),\n",
    "        #     ('classifier', classifier)\n",
    "        #                  ])\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:-1], df.iloc[:,-1], test_size=0.2, random_state=42)\n",
    "        \n",
    "        # pipe.fit(X_train, y_train)\n",
    "        # return pipe.score(X_test, y_test)\n",
    "        \n",
    "        x = df.iloc[:,:-1]\n",
    "        y = df.iloc[:,-1]\n",
    "        # y = LabelEncoder().fit_transform(y)\n",
    "        \n",
    "        # X_train, X_test, y_train, y_test = train_test_split(\n",
    "        #                                                         x,\n",
    "        #                                                         y,\n",
    "        #                                                         test_size=1/3,\n",
    "        #                                                         random_state=0)\n",
    "        # classifier.fit(X_train, y_train)\n",
    "                                                                \n",
    "        # return classifier.score(X_test, y_test)\n",
    "        \n",
    "\n",
    "        # # # evaluate the model\n",
    "        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "        n_scores = cross_val_score(classifier, x, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "        return np.mean(n_scores)\n",
    "        \n",
    "        \n",
    "\n",
    "class Vehicle(DatasetPart3):\n",
    "    def __init__(self, df):\n",
    "        super().__init__(df)\n",
    "    \n",
    "    @classmethod\n",
    "    def constructFromFile(cls, filePath):\n",
    "        df = pd.read_csv(filePath, header=None, delim_whitespace=True)\n",
    "        df.columns = [f\"f_{i}\" for i in range(len(df.columns))]\n",
    "        df.rename(columns = {f'f_{len(df.columns)-1}':'class'}, inplace = True)\n",
    "        return cls(df)\n",
    "    \n",
    "class MuskClean(DatasetPart3):\n",
    "    def __init__(self, df):\n",
    "        super().__init__(df)\n",
    "\n",
    "    @classmethod\n",
    "    def constructFromFile(cls, filePath):\n",
    "        df = pd.read_csv(filePath, header=None)\n",
    "        # ignore the first 2 columns since they are NOT numerical, so it would be betteer to ignore them \n",
    "        df.drop([0,1], axis=1, inplace=True)\n",
    "        df.columns = [f\"f_{i}\" for i in range(len(df.columns))]\n",
    "        df.rename(columns = {f'f_{len(df.columns)-1}':'class'}, inplace = True)\n",
    "        return cls(df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_vehicle = Vehicle.constructFromFile(\"./vehicle/vehicle.dat\")\n",
    "\n",
    "# ds_vehicle.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_mushclean = MuskClean.constructFromFile(\"./musk/clean1.data\")\n",
    "# ds_mushclean.df\n",
    "# # len(ds_mushclean.x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/pkg/lib/python3.9/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/usr/pkg/lib/python3.9/site-packages/deap/creator.py:138: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "# 2 minimum objectives, so -1,-1\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0, -1.0)) \n",
    "# Individual should be a list of binary values, i.e. a list of 0s and 1s\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define wrapper based fitness evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getTransformedDf(df2Transform:pd.DataFrame):\n",
    "#     \"\"\"transform the continous features to discontinous. In other words, due to all features are continous, this functions are used to discretise all continous features.\n",
    "\n",
    "#     KBins is used to discretise the continous features. The number of bins is set to 10. The strategy is set to uniform.\n",
    "    \n",
    "#     Tutorial: https://machinelearningmastery.com/discretization-transforms-for-machine-learning/\n",
    "    \n",
    "#     Args:\n",
    "#         df2Transform (pd.DataFrame): df to transform, all features should be continous\n",
    "        \n",
    "#     \"\"\" \n",
    "#     tempDf = deepcopy(df2Transform)\n",
    "#     tempDf_x = tempDf.iloc[:,:-1]\n",
    "#     tempDf_y = tempDf.iloc[:,-1]\n",
    "#     # tempDf_y = LabelEncoder().fit_transform(tempDf_y)\n",
    "#     # only transform the continous features, ignore Y\n",
    "#     kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "#     tempDf_x = kbins.fit_transform(tempDf_x)\n",
    "#     tempDf = pd.concat([pd.DataFrame(tempDf_x),tempDf_y],axis=1)\n",
    "#     tempDf.columns = [f\"f_{i}\" for i in range(len(tempDf.columns))]\n",
    "#     tempDf.rename(columns = {f'f_{len(tempDf.columns)-1}':'class'}, inplace = True)\n",
    "#     return tempDf\n",
    "\n",
    "def wrapperFitnessEvaluation(ds:DatasetPart3, individual:creator.Individual, \n",
    "                             classifier=CONSTANTS_DICT[\"CLASSIFIER\"]): #KNN by default\n",
    "    df_selected,selected_count = ds.getDfWithSelectedFeatures(individual)\n",
    "    # df_selected = getTransformedDf(df_selected)\n",
    "        \n",
    "    acc_score = DatasetPart3.run_model(df_selected, classifier)\n",
    "    obj1 = 1.0-acc_score # classification error\n",
    "    obj2 = selected_count/len(individual) #ratio of selected features\n",
    "    return obj1, obj2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tool box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toolbox is a class contains the operators that we will use in our genetic programming algorithm\n",
    "# it can be also be used as the container of methods which enables us to add new methods to the toolbox \n",
    "def setup_toolbox(ds:DatasetPart3, randSeed:int, evaluateFunction=wrapperFitnessEvaluation) -> base.Toolbox:\n",
    "    toolbox = base.Toolbox()\n",
    "    # for population size, we use the random.randint function to generate a random integer in the range [min, max]\n",
    "    random.seed(randSeed)\n",
    "    # register a method to generate random boolean values\n",
    "    toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "    # register a method to generate random individuals\n",
    "    toolbox.register(\"IndividualCreator\", \n",
    "                     tools.initRepeat, \n",
    "                     creator.Individual, \n",
    "                     toolbox.attr_bool, \n",
    "                     n=len(ds.x.columns) # feature number, exclude the class column\n",
    "                    )\n",
    "    \n",
    "    # N is not specificied, so need to specify number of individuals to generate within each population when we call it later\n",
    "    toolbox.register(\"PopulationCreator\", tools.initRepeat, list, toolbox.IndividualCreator) \n",
    "    toolbox.register(\"select\", tools.selNSGA2)\n",
    "    # toolbox.register(\"mate\", tools.cxSimulatedBinaryBounded, low=CONSTANTS_DICT[\"BOUND_LOW\"], up=CONSTANTS_DICT[\"BOUND_UP\"], eta=20.0)\n",
    "    # toolbox.register(\"mutate\", tools.mutPolynomialBounded, low=CONSTANTS_DICT[\"BOUND_LOW\"], up=CONSTANTS_DICT[\"BOUND_UP\"], eta=20.0, indpb=1.0/len(ds.x.columns))\n",
    "    \n",
    "    # toolbox.register(\"elitism\", tools.\n",
    "    \n",
    "    # toolbox.register(\"elitism\", tools.selBest, k=int(CONSTANTS_DICT[\"ELITIST_PERCENTAGE\"]*ds.M))\n",
    "    # # toolbox.register(\"select\", tools.selTournament, k=2, tournsize=3)\n",
    "    \n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint) # TODO: might need to change this to cxOnePoint\n",
    "    # indpb refer to the probability of mutate happening on each gene, it is NOT the same as mutation rate\n",
    "    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=1.0/ds.M) \n",
    "    \n",
    "    toolbox.register(\"evaluate\", evaluateFunction, ds) # need to pass individual:list\n",
    "    return toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run NSGA once \n",
    "\n",
    "> https://github.dev/DEAP/deap/blob/master/deap/tools/emo.py\n",
    "> https://github.dev/DEAP/deap/blob/master/examples/ga/nsga2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from select import select\n",
    "import time\n",
    "\n",
    "def run_NSGAII(ds:DatasetPart3, randSeed:int, \n",
    "                ngen:int=CONSTANTS_DICT[\"MAX_GENERATIONS\"], \n",
    "                popSize:int=CONSTANTS_DICT[\"POPULATION_SIZE\"]):\n",
    "    # stats\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min, axis=0)\n",
    "    stats.register(\"max\", np.max, axis=0)\n",
    "    stats.register(\"mean\", np.mean, axis = 0)\n",
    "    stats.register(\"std\", np.std, axis=0)\n",
    "    # for record keeping\n",
    "    logbook = tools.Logbook()    \n",
    "    logbook.header = \"gen\", \"mean\", \"std\", \"min\",  \"max\"\n",
    "    \n",
    "    # create toolbox\n",
    "    random.seed(randSeed)\n",
    "    toolbox = setup_toolbox(ds, randSeed)\n",
    "    # create the initial population\n",
    "    population = toolbox.PopulationCreator(n=popSize)\n",
    "    \n",
    "    # calculate objectives\n",
    "    def evaluate_fitness_values(pop) :\n",
    "        \"\"\"Update the fitness values of each individual for the given the population\"\"\"\n",
    "        # invalid_ind = [ind for ind in pop if not ind.fitness.valid]\n",
    "        # fitnesses =toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, pop)\n",
    "        # print(f\"fitnesses: {fitnesses}\")\n",
    "\n",
    "        for ind, fit in zip(pop, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "    evaluate_fitness_values(population)\n",
    "    \n",
    "    # This is just to assign the crowding distance to the individuals\n",
    "    # no actual selection is done\n",
    "    population = toolbox.select(population, len(population))\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen_counter in range(ngen):\n",
    "        \n",
    "        # Vary the population\n",
    "        offspring = tools.selTournamentDCD(population, len(population))\n",
    "        offspring = [toolbox.clone(ind) for ind in offspring]\n",
    "        \n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for ind1, ind2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() <= CONSTANTS_DICT[\"CROSSOVER_RATE\"]: # always crossover\n",
    "                toolbox.mate(ind1, ind2)\n",
    "            if random.random() <= CONSTANTS_DICT[\"MUTATION_RATE\"]:\n",
    "                toolbox.mutate(ind1)\n",
    "                toolbox.mutate(ind2)\n",
    "            # # print(ind1,ind2)\n",
    "\n",
    "            # del ind1.fitness.values\n",
    "            # del ind2.fitness.values\n",
    "                \n",
    "                # del ind1.fitness.values\n",
    "                # del ind2.fitness.values\n",
    "        # for mutant in offspring:\n",
    "        #     if random.random() <= CONSTANTS_DICT[\"MUTATION_RATE\"]:\n",
    "        #         toolbox.mutate(mutant)\n",
    "                # del mutant.fitness.values\n",
    "                \n",
    "        # Evaluate all  offsprings individuals \n",
    "        evaluate_fitness_values(offspring)\n",
    "\n",
    "      \n",
    "        # elitism strategy\n",
    "        # Select the next generation population\n",
    "        population = toolbox.select(population + offspring, popSize)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # stats\n",
    "        record = stats.compile(population)\n",
    "        logbook.record(gen=gen_counter,  **record)\n",
    "        print(logbook.stream)\n",
    "        \n",
    "    print(\"Final population hypervolume is %f\" % hypervolume(population, [11.0, 11.0]))\n",
    "    return population, logbook, hypervolume(population, [11.0, 11.0]) # set of non-dominated individuals solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def run_3_times_with_different_seed(ds:DatasetPart3,\n",
    "                                     title:str, \n",
    "                                     max_gen=CONSTANTS_DICT[\"MAX_GENERATIONS\"],\n",
    "                                     classifier = CONSTANTS_DICT[\"CLASSIFIER\"],\n",
    "                                     randSeed = [i for i in range(3)],\n",
    "                                     run_times=3):\n",
    "    # run 3 times with different seed\n",
    "    population_list = []\n",
    "    logbook_list = []\n",
    "    hypervolume_list = []\n",
    "    \n",
    "    for i in range(run_times):\n",
    "        print('-'*80)\n",
    "        print('-'*80)\n",
    "        print(title,\"\\nRunning GA with seed: \", randSeed[i])\n",
    "        population, logbook, hypervolume = run_NSGAII(ds, randSeed=randSeed[i], ngen=max_gen, popSize=CONSTANTS_DICT[\"POPULATION_SIZE\"])\n",
    "        population_list.append(population)\n",
    "        logbook_list.append(logbook)\n",
    "        hypervolume_list.append(hypervolume)    \n",
    "        print('-'*80)\n",
    "        print('-'*80)\n",
    "        \n",
    "        # plot the result\n",
    "        fitTuple = [ind.fitness.values for ind in population]\n",
    "            \n",
    "        plt.plot(fitTuple[0], fitTuple[1], label=f\"seed {randSeed[i]}\\n hypervolume: {hypervolume}\")\n",
    "        plt.legend(bbox_to_anchor =(1.3,-0.1), loc='lower center')\n",
    "        plt.ylabel(\"ratio of selected features\")\n",
    "        plt.xlabel(\"classification error rate\")\n",
    "        plt.title(f\"dataset: {title} \\n Objective space\")\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "    # compare error rates of the obtained solutions with that of using the entire feature set.\n",
    "    subset_mean_err_rate = [logbook.select(\"mean\")[0] for logbook in logbook_list]\n",
    "    entire_mean_err_rate = DatasetPart3.run_model(ds.df)\n",
    "    \n",
    "    print(f\"{title}:\\n error rates of the obtained solution: {subset_mean_err_rate}\\n error rate of using the entire feature set: {entire_mean_err_rate}\")\n",
    "        \n",
    "\n",
    "    return population_list, logbook_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "vehicle \n",
      "Running GA with seed:  0\n",
      "gen\tmean                   \tstd                    \tmin                    \tmax                    \n",
      "0  \t[0.35807311 0.43388889]\t[0.05246388 0.1153885 ]\t[0.29148926 0.11111111]\t[0.54964052 0.72222222]\n",
      "1  \t[0.35039991 0.39777778]\t[0.06231751 0.13140269]\t[0.2844211  0.11111111]\t[0.58433707 0.72222222]\n",
      "2  \t[0.34692923 0.36833333]\t[0.07012796 0.15548709]\t[0.2844211  0.11111111]\t[0.58433707 0.66666667]\n",
      "3  \t[0.34070677 0.37      ]\t[0.07372455 0.17395544]\t[0.27934641 0.05555556]\t[0.64927638 0.66666667]\n",
      "4  \t[0.35860756 0.32944444]\t[0.09616896 0.17972115]\t[0.27888422 0.05555556]\t[0.73873483 0.66666667]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/pkg/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 436, in _process_worker\n    r = call_item()\n  File \"/usr/pkg/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 288, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/usr/pkg/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/pkg/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/usr/pkg/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/usr/pkg/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 515, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/pkg/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 1130, in fit\n    X, y = check_X_y(X, y, \"csr\", multi_output=True)\n  File \"/usr/pkg/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 747, in check_X_y\n    X = check_array(X, accept_sparse=accept_sparse,\n  File \"/usr/pkg/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 475, in check_array\n    dtype_orig = np.result_type(*dtypes_orig)\n  File \"<__array_function__ internals>\", line 180, in result_type\nValueError: at least one array or dtype is required\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=0'>1</a>\u001b[0m ds_vehicle \u001b[39m=\u001b[39m Vehicle\u001b[39m.\u001b[39mconstructFromFile(\u001b[39m\"\u001b[39m\u001b[39m./vehicle/vehicle.dat\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=1'>2</a>\u001b[0m run_3_times_with_different_seed(ds_vehicle, \u001b[39m\"\u001b[39;49m\u001b[39mvehicle\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=2'>3</a>\u001b[0m                                 max_gen\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=3'>4</a>\u001b[0m                                 run_times\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb Cell 16\u001b[0m in \u001b[0;36mrun_3_times_with_different_seed\u001b[0;34m(ds, title, max_gen, classifier, randSeed, run_times)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m80\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(title,\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mRunning GA with seed: \u001b[39m\u001b[39m\"\u001b[39m, randSeed[i])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=16'>17</a>\u001b[0m population, logbook, hypervolume \u001b[39m=\u001b[39m run_NSGAII(ds, randSeed\u001b[39m=\u001b[39;49mrandSeed[i], ngen\u001b[39m=\u001b[39;49mmax_gen, popSize\u001b[39m=\u001b[39;49mCONSTANTS_DICT[\u001b[39m\"\u001b[39;49m\u001b[39mPOPULATION_SIZE\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=17'>18</a>\u001b[0m population_list\u001b[39m.\u001b[39mappend(population)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=18'>19</a>\u001b[0m logbook_list\u001b[39m.\u001b[39mappend(logbook)\n",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb Cell 16\u001b[0m in \u001b[0;36mrun_NSGAII\u001b[0;34m(ds, randSeed, ngen, popSize)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=52'>53</a>\u001b[0m         toolbox\u001b[39m.\u001b[39mmutate(ind2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=53'>54</a>\u001b[0m     \u001b[39m# # print(ind1,ind2)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=54'>55</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=55'>56</a>\u001b[0m     \u001b[39m# del ind1.fitness.values\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=64'>65</a>\u001b[0m         \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=65'>66</a>\u001b[0m \u001b[39m# Evaluate all  offsprings individuals \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=66'>67</a>\u001b[0m evaluate_fitness_values(offspring)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=69'>70</a>\u001b[0m \u001b[39m# elitism strategy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=70'>71</a>\u001b[0m \u001b[39m# Select the next generation population\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=71'>72</a>\u001b[0m population \u001b[39m=\u001b[39m toolbox\u001b[39m.\u001b[39mselect(population \u001b[39m+\u001b[39m offspring, popSize)\n",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb Cell 16\u001b[0m in \u001b[0;36mrun_NSGAII.<locals>.evaluate_fitness_values\u001b[0;34m(pop)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=28'>29</a>\u001b[0m fitnesses \u001b[39m=\u001b[39m toolbox\u001b[39m.\u001b[39mmap(toolbox\u001b[39m.\u001b[39mevaluate, pop)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=29'>30</a>\u001b[0m \u001b[39m# print(f\"fitnesses: {fitnesses}\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m ind, fit \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(pop, fitnesses):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=32'>33</a>\u001b[0m     ind\u001b[39m.\u001b[39mfitness\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m fit\n",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb Cell 16\u001b[0m in \u001b[0;36mwrapperFitnessEvaluation\u001b[0;34m(ds, individual, classifier)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=25'>26</a>\u001b[0m df_selected,selected_count \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mgetDfWithSelectedFeatures(individual)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=26'>27</a>\u001b[0m \u001b[39m# df_selected = getTransformedDf(df_selected)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=28'>29</a>\u001b[0m acc_score \u001b[39m=\u001b[39m DatasetPart3\u001b[39m.\u001b[39;49mrun_model(df_selected, classifier)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=29'>30</a>\u001b[0m obj1 \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\u001b[39m-\u001b[39macc_score \u001b[39m# classification error\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=30'>31</a>\u001b[0m obj2 \u001b[39m=\u001b[39m selected_count\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(individual) \u001b[39m#ratio of selected features\u001b[39;00m\n",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb Cell 16\u001b[0m in \u001b[0;36mDatasetPart3.run_model\u001b[0;34m(df, classifier)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=43'>44</a>\u001b[0m \u001b[39m# y = LabelEncoder().fit_transform(y)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=44'>45</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=45'>46</a>\u001b[0m \u001b[39m# X_train, X_test, y_train, y_test = train_test_split(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=54'>55</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=55'>56</a>\u001b[0m \u001b[39m# # # evaluate the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=56'>57</a>\u001b[0m cv \u001b[39m=\u001b[39m RepeatedStratifiedKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, n_repeats\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=57'>58</a>\u001b[0m n_scores \u001b[39m=\u001b[39m cross_val_score(classifier, x, y, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m, cv\u001b[39m=\u001b[39;49mcv, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, error_score\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mraise\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p3/p3.ipynb#ch0000018?line=58'>59</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(n_scores)\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:385\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    383\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 385\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(estimator\u001b[39m=\u001b[39;49mestimator, X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    386\u001b[0m                             scoring\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m'\u001b[39;49m: scorer}, cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    387\u001b[0m                             n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    388\u001b[0m                             fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    389\u001b[0m                             pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    390\u001b[0m                             error_score\u001b[39m=\u001b[39;49merror_score)\n\u001b[1;32m    391\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m'\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:230\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    229\u001b[0m                     pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 230\u001b[0m scores \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    231\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    232\u001b[0m         clone(estimator), X, y, scorers, train, test, verbose, \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    233\u001b[0m         fit_params, return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    234\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    235\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score)\n\u001b[1;32m    236\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    238\u001b[0m zipped_scores \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mscores))\n\u001b[1;32m    239\u001b[0m \u001b[39mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1057\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    936\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/concurrent/futures/_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    445\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "ds_vehicle = Vehicle.constructFromFile(\"./vehicle/vehicle.dat\")\n",
    "run_3_times_with_different_seed(ds_vehicle, \"vehicle\",\n",
    "                                max_gen=10,\n",
    "                                run_times=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mushclean = MuskClean.constructFromFile(\"./musk/clean1.data\")\n",
    "run_3_times_with_different_seed(ds_vehicle, \"mushclean\",\n",
    "                                max_gen=10,\n",
    "                                run_times=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52da647447b8fe2208076266408c42f82750713fb5b92055dee0a0742687bf52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
