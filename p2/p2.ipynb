{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reading the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from distutils.command.build_scripts import first_line_re\n",
    "from tkinter.tix import COLUMN\n",
    "# Import deque for the stack structure, copy for deep copy nodes\n",
    "from collections import deque\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import (DecisionTreeClassifier, DecisionTreeRegressor,\n",
    "                          ExtraTreeClassifier)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "# Encoding categorical features with preserving the missing values in incomplete features\n",
    "from sklearn.preprocessing import (KBinsDiscretizer, LabelEncoder,\n",
    "                                   OneHotEncoder, OrdinalEncoder,\n",
    "                                   StandardScaler)\n",
    "\n",
    "# define some constants for the genetic algorithm\n",
    "CONSTANTS_DICT = {\n",
    "    \"POPULATION_SIZE\": 100, # number of individuals in each population\n",
    "    \"MAX_GENERATIONS\": 250, # number of generations to run the algorithm\n",
    "    \"CROSSOVER_RATE\": 1.0, # crossover rate should always be 100%, based on slides\n",
    "    \"MUTATION_RATE\": 0.2, # mutation rate\n",
    "    \"ELITIST_PERCENTAGE\": 0.05, # percentage of the best individuals to keep in the next generation\n",
    "    \"CLASSIFIER\": DecisionTreeClassifier(criterion='entropy'), # classifier to use\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot\n",
    "class DatasetPart2:\n",
    "    \n",
    "    def summarize_ds(self):\n",
    "        print(self.df.shape)\n",
    "        # summarize each variable\n",
    "        print(self.df.describe())\n",
    "        # histograms of the variables\n",
    "        self.df.hist()\n",
    "        pyplot.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def run_model(df:pd.DataFrame, classifier):\n",
    "        x = df.iloc[:,:-1]\n",
    "        y = df.iloc[:,-1]\n",
    "\n",
    "        \n",
    "        # evaluate the model\n",
    "        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "        n_scores = cross_val_score(classifier, x, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "        # report model performance\n",
    "        \n",
    "        \n",
    "        # classifier.fit(x,y)\n",
    "        # acc_score = accuracy_score(y, classifier.predict(x))\n",
    "        return np.mean(n_scores)\n",
    "\n",
    "    \n",
    "    def __init__(self,df):\n",
    "        self.df=df\n",
    "        self.x = self.df.iloc[:,:-1]\n",
    "        self.y = self.df.iloc[:,-1]\n",
    "        self.M = self.df.shape[0]  # number of rows\n",
    "        \n",
    "        \n",
    "        # self.df_transformed = self.getTransformedDf(self.df)\n",
    "        # self.x_transformed = self.df_transformed.iloc[:,:-1]\n",
    "        # self.y_transformed = self.df_transformed.iloc[:,-1]\n",
    "        # self.y = self.df.iloc[:-1]\n",
    "\n",
    "    @classmethod\n",
    "    def constructFromFile(cls,filePath):\n",
    "        df = pd.read_csv(filePath,header=None)\n",
    "        df.columns = [f\"f_{i}\" for i in range(len(df.columns))]\n",
    "        df.rename(columns = {f'f_{len(df.columns)-1}':'class'}, inplace = True)\n",
    "        return cls(df) \n",
    "    \n",
    "    def getDfWithSelectedFeatures(self, selectedFeatures:list):\n",
    "        returnedDf = pd.DataFrame()\n",
    "        for i in range(len(selectedFeatures)):\n",
    "            isSelected = True if selectedFeatures[i] == 1 else False\n",
    "            if isSelected:\n",
    "                # concat this feature to the returned dataframe\n",
    "                returnedDf = pd.concat([returnedDf,self.df.iloc[:,i]],axis=1)\n",
    "        # concat the class column\n",
    "        returnedDf = pd.concat([returnedDf,self.df.iloc[:,-1]],axis=1)\n",
    "        return returnedDf\n",
    "    \n",
    "    @staticmethod\n",
    "    def getTransformedDf(df2Transform:pd.DataFrame):\n",
    "        \"\"\"transform the continous features to discontinous. In other words, due to all features are continous, this functions are used to discretise all continous features.\n",
    "\n",
    "        KBins is used to discretise the continous features. The number of bins is set to 10. The strategy is set to uniform.\n",
    "        \n",
    "        Tutorial: https://machinelearningmastery.com/discretization-transforms-for-machine-learning/\n",
    "        \n",
    "        Args:\n",
    "            df2Transform (pd.DataFrame): df to transform, all features should be continous\n",
    "            \n",
    "        \"\"\" \n",
    "        tempDf = deepcopy(df2Transform)\n",
    "        tempDf_x = tempDf.iloc[:,:-1]\n",
    "        tempDf_y = tempDf.iloc[:,-1]\n",
    "        # only transform the continous features, ignore Y\n",
    "        kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "        tempDf_x = kbins.fit_transform(tempDf_x)\n",
    "        tempDf = pd.concat([pd.DataFrame(tempDf_x),tempDf_y],axis=1)\n",
    "        tempDf.columns = [f\"f_{i}\" for i in range(len(tempDf.columns))]\n",
    "        tempDf.rename(columns = {f'f_{len(tempDf.columns)-1}':'class'}, inplace = True)\n",
    "        return tempDf\n",
    "        \n",
    "class Sonar(DatasetPart2):\n",
    "    def __init__(self,df):\n",
    "        super().__init__(df)\n",
    "    \n",
    "class Wbcd(DatasetPart2):\n",
    "    def __init__(self,df):\n",
    "        super().__init__(df)\n",
    "\n",
    "ds_sonar  = Sonar.constructFromFile(\"./sonar/sonar.data\")\n",
    "ds_wbcd = Wbcd.constructFromFile(\"./wbcd/wbcd.data\")\n",
    "\n",
    "# # ds_sonar.getDfWithSelectedFeatures([0,1,1,1,1,0])\n",
    "# ds_sonar.getTransformedDf(ds_sonar.df)\n",
    "# ds_sonar.summarize_ds()\n",
    "# ds_sonar.df\n",
    "\n",
    "\n",
    "ds_sonar.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>f_9</th>\n",
       "      <th>...</th>\n",
       "      <th>f_51</th>\n",
       "      <th>f_52</th>\n",
       "      <th>f_53</th>\n",
       "      <th>f_54</th>\n",
       "      <th>f_55</th>\n",
       "      <th>f_56</th>\n",
       "      <th>f_57</th>\n",
       "      <th>f_58</th>\n",
       "      <th>f_59</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        f_0     f_1     f_2     f_3     f_4     f_5     f_6     f_7     f_8  \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "        f_9  ...    f_51    f_52    f_53    f_54    f_55    f_56    f_57  \\\n",
       "0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
       "204  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
       "205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
       "206  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
       "207  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
       "\n",
       "       f_58    f_59  class  \n",
       "0    0.0090  0.0032      1  \n",
       "1    0.0052  0.0044      1  \n",
       "2    0.0095  0.0078      1  \n",
       "3    0.0040  0.0117      1  \n",
       "4    0.0107  0.0094      1  \n",
       "..      ...     ...    ...  \n",
       "203  0.0193  0.0157      2  \n",
       "204  0.0062  0.0067      2  \n",
       "205  0.0077  0.0031      2  \n",
       "206  0.0036  0.0048      2  \n",
       "207  0.0061  0.0115      2  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_sonar.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use **Creator** to define the type of individuals and fitness classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import constants\n",
    "from json import tool\n",
    "from deap import creator, base, gp, tools, algorithms # core functionality of DEAP\n",
    "import array\n",
    "import random\n",
    "import json\n",
    "import math # for checking the fitness of an individual, i.e. math.isinf(weight)\n",
    "import matplotlib.pyplot as plt\n",
    "# Python internal operators for object comparisons, \n",
    "# logical comparisons, arithmetic operations, and sequence operations\n",
    "import operator \n",
    "\n",
    "# creator is  usually used to define the type of the individual and fitness classes\n",
    "\n",
    "# goal:to maximize the value and do not exceed the capacity of the knapsack\n",
    "# define strategies with different priorities for optimizing multiple goals by using FitnessCompound\n",
    "# 1 for maximize value, -1 for minimize weight, \n",
    "# creator.create(\"FitnessCompound\", base.Fitness, weights=(1.0,-1.0)) \n",
    " \n",
    "# according to slide, fitness value has been reduced to 1 dimension, so just use FitnessMax\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "# Individual should be a list of binary values, i.e. a list of 0s and 1s\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define the evaluate function for **FilterGA** and **WrapperGA**.\n",
    "> \n",
    "> Inspired by the slide, and https://datascience.stackexchange.com/questions/58565/conditional-entropy-calculation-in-python-hyx\n",
    ">\n",
    "> code is from https://datascience.stackexchange.com/questions/58565/conditional-entropy-calculation-in-python-hyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Entropy\n",
    "def entropy(Y):\n",
    "    \"\"\"\n",
    "    Also known as Shanon Entropy\n",
    "    Reference: https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "    \"\"\"\n",
    "    unique, count = np.unique(Y, return_counts=True, axis=0)\n",
    "    prob = count/len(Y)\n",
    "    en = np.sum((-1)*prob*np.log2(prob))\n",
    "    return en\n",
    "    # my implementation, it is the same, \n",
    "    # entropy_y = 0\n",
    "    # for i in range(len(Y.unique())):\n",
    "    #     p_y = len(Y[Y==Y.unique()[i]])/len(Y)\n",
    "    #     entropy_y += -p_y*math.log2(p_y)\n",
    "    # return entropy_y\n",
    "    \n",
    "# print(entropy(ds_sonar.y))\n",
    "\n",
    "#Joint Entropy\n",
    "def jEntropy(Y,X):\n",
    "    \"\"\"\n",
    "    H(Y;X)\n",
    "    Reference: https://en.wikipedia.org/wiki/Joint_entropy\n",
    "    \"\"\"\n",
    "    YX = np.c_[Y,X]\n",
    "    return entropy(YX)\n",
    "\n",
    "#Conditional Entropy\n",
    "def cEntropy(Y, X):\n",
    "    \"\"\"\n",
    "    conditional entropy = Joint Entropy - Entropy of X\n",
    "    H(Y|X) = H(Y;X) - H(X)\n",
    "    Reference: https://en.wikipedia.org/wiki/Conditional_entropy\n",
    "    \"\"\"\n",
    "    return jEntropy(Y, X) - entropy(X)\n",
    "\n",
    "\n",
    "#Information Gain\n",
    "def gain(Y, X):\n",
    "    \"\"\"\n",
    "    Information Gain, I(Y;X) = H(Y) - H(Y|X)\n",
    "    Reference: https://en.wikipedia.org/wiki/Information_gain_in_decision_trees#Formal_definition\n",
    "    \"\"\"\n",
    "    # return entropy(Y) - cEntropy(Y,X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def evaluateFilterGA(ds:DatasetPart2, individual:creator.Individual): \n",
    "    \"\"\"Goodness of a individual(i.e. feature subset) independent of the classifier.\n",
    "    Information Gain is used to evaluate the goodness of a feature subset.\n",
    "\n",
    "    Args:\n",
    "        ds (DatasetPart2): Given ds, for which the individual is evaluated\n",
    "        individual (creator.Individual): _description_\n",
    "    \"\"\" \n",
    "    # get the df with selected features, StandardScaler is already used to scale continous features into the discrete values\n",
    "    df_selected = ds.getDfWithSelectedFeatures(individual)\n",
    "    df_selected_transformed = DatasetPart2.getTransformedDf(df_selected)\n",
    "    y = df_selected.iloc[:,-1]\n",
    "    x = df_selected.iloc[:,:-1]\n",
    "    \n",
    "    info_gain = gain(y,x) # I(Y;X) = H(Y) - H(Y|X)\n",
    "    info_gain_ratio = info_gain/entropy(x) # I(Y;X)/H(X)\n",
    "    \n",
    "    return info_gain, #info_gain_ratio,\n",
    "\n",
    "def evaluateWrapperGA(ds:DatasetPart2, individual:creator.Individual, classifier = CONSTANTS_DICT[\"CLASSIFIER\"]): \n",
    "    df_selected = ds.getDfWithSelectedFeatures(individual)\n",
    "    df_selected_transformed = DatasetPart2.getTransformedDf(df_selected)\n",
    "    acc_score = DatasetPart2.run_model(df_selected_transformed, classifier)\n",
    "    return acc_score,    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup toolbox for registering the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# toolbox is a class contains the operators that we will use in our genetic programming algorithm\n",
    "# it can be also be used as the container of methods which enables us to add new methods to the toolbox \n",
    "def setup_toolbox(ds:DatasetPart2, evaluateFunction,randSeed:int) -> base.Toolbox:\n",
    "    toolbox = base.Toolbox()\n",
    "    # for population size, we use the random.randint function to generate a random integer in the range [min, max]\n",
    "    random.seed(randSeed)\n",
    "    # register a method to generate random boolean values\n",
    "    toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "    # register a method to generate random individuals\n",
    "    toolbox.register(\"IndividualCreator\", \n",
    "                     tools.initRepeat, \n",
    "                     creator.Individual, \n",
    "                     toolbox.attr_bool, \n",
    "                     n=len(ds.x.columns) # feature number, exclude the class column\n",
    "                    )\n",
    "    \n",
    "    # N is not specificied, so need to specify number of individuals to generate within each population when we call it later\n",
    "    toolbox.register(\"PopulationCreator\", tools.initRepeat, list, toolbox.IndividualCreator) \n",
    "    \n",
    "    toolbox.register(\"elitism\", tools.selBest, k=int(CONSTANTS_DICT[\"ELITIST_PERCENTAGE\"]*ds.M))\n",
    "    toolbox.register(\"select\", tools.selTournament, k=2, tournsize=3)\n",
    "    \n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint) # TODO: might need to change this to cxOnePoint\n",
    "    # indpb refer to the probability of mutate happening on each gene, it is NOT the same as mutation rate\n",
    "    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=1.0/ds.M) # TODO: might need to change this to mutUniformInt\n",
    "    # local search operator\n",
    "    # toolbox.register(\"local_search\", algorithms)\n",
    "    \n",
    "    \n",
    "    # register the evaluation function\n",
    "    toolbox.register(\"evaluate\", evaluateFunction, ds) # register a method to evaluate the fitness of an individual\n",
    "    return toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "from select import select\n",
    "import time\n",
    "\n",
    "def run_GA_framework(ds:DatasetPart2,evaluateFunction, max_gen= CONSTANTS_DICT[\"MAX_GENERATIONS\"], randSeed:int=1) -> creator.Individual:\n",
    "    '''\n",
    "    Run the genetic algorithm framework\n",
    "    '''\n",
    "    # for toolbox\n",
    "    random.seed(randSeed)\n",
    "    toolbox = setup_toolbox(ds,evaluateFunction, randSeed)\n",
    "    # for record keeping\n",
    "    logbook = tools.Logbook()    \n",
    "    # assign the stats for recording the computational time\n",
    "    # stats = tools.Statistics()\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"mean\", np.mean, axis = 0)\n",
    "    stats.register(\"std\", np.std, axis=0)\n",
    "  \n",
    "    # create the initial population\n",
    "    population = toolbox.PopulationCreator(n=CONSTANTS_DICT[\"POPULATION_SIZE\"])\n",
    "    \n",
    "    # # evaluate the fitness of the current population, and assign the fitness to each individual\n",
    "    # evaluate_fitness_values(population)\n",
    "    def evaluate_fitness_values(pop) :\n",
    "        \"\"\"Update the fitness values of each individual for the given the population\"\"\"\n",
    "        fitnesses = list(map(toolbox.evaluate, pop))\n",
    "        for ind, fit in zip(pop, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "    \n",
    "    \n",
    "    best_feasible_individual = None\n",
    "    # computation_time_list = [] \n",
    "    startTime = time.time()\n",
    "    # start the evolution\n",
    "    for gen_counter in range(max_gen):\n",
    "        # assign the fitness to each individual in the current generation \n",
    "        evaluate_fitness_values(population)\n",
    "        # for recording the time spent on each generation\n",
    "        genStartTime = time.time()\n",
    "        # # time_cost = computation_time_list[-1] - genStartTime if len(computation_time_list) > 0 \\\n",
    "                                                            #  else 0.0\n",
    "        # computation_time_list.append(time_cost)\n",
    "        # for visualizing whether the algorithm is still running\n",
    "        best_feasible_individual = tools.selBest(population, k=5)[0]\n",
    "        best_fintess_current_gen = best_feasible_individual.fitness.values[0]\n",
    "        # record the statistics of the current generation\n",
    "        # record = stats.compile(computation_time_list) \n",
    "        # logbook.record(gen=gen_counter,\n",
    "        #                best_fintess_current_gen=best_fintess_current_gen, best_ind_chromosome=best_feasible_individual,computation_time_list=computation_time_list,\n",
    "        #                **record)\n",
    "        record = stats.compile(population)\n",
    "        logbook.record(gen=gen_counter,\n",
    "                       best_fintess_current_gen=best_fintess_current_gen, best_ind_chromosome=best_feasible_individual,\n",
    "                       **record)\n",
    "        \n",
    "        \n",
    "        # apply elitism to obtain the best individuals in the current generation\n",
    "        offspring = toolbox.elitism(population)\n",
    "\n",
    "        # repeat until the offspring has the same size as the population\n",
    "        while len(offspring) < CONSTANTS_DICT[\"POPULATION_SIZE\"]:\n",
    "            # apply selection\n",
    "            parent1,parent2 = toolbox.select(population)\n",
    "\n",
    "            # apply crossover\n",
    "            c1,c2 = toolbox.mate(copy.deepcopy(parent1),copy.deepcopy(parent2))\n",
    "            \n",
    "            # apply mutation to the children\n",
    "            for child in [c1,c2]:\n",
    "                if random.random() < CONSTANTS_DICT[\"MUTATION_RATE\"]:\n",
    "                    toolbox.mutate(child)\n",
    "                    del child.fitness.values\n",
    "                # append the children to the offspring\n",
    "            # TODO: apply local search to the children\n",
    "            # annoying, time consuming, not implemented yet although Yi's github got tutorial\n",
    "\n",
    "            offspring.append(c1)\n",
    "            offspring.append(c2)\n",
    "        # replace the current population with the offspring new gwneration\n",
    "        population[:] = offspring\n",
    "        \n",
    "    timeSpent = time.time() - startTime\n",
    "    return best_feasible_individual, logbook, stats, timeSpent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from cProfile import label\n",
    "\n",
    "\n",
    "def run_5_times_with_different_seeds(ds:DatasetPart2,\n",
    "                                     title:str, \n",
    "                                     evaluateFunction,\n",
    "                                     classifier = CONSTANTS_DICT[\"CLASSIFIER\"],\n",
    "                                     max_gen=CONSTANTS_DICT[\"MAX_GENERATIONS\"],\n",
    "                                     randSeed = [i+1 for i in range(5)],\n",
    "                                     run_times=5):\n",
    "    '''\n",
    "    Run the GA framework 5 times with different seeds\n",
    "    '''\n",
    "    five_computional_time_list = []\n",
    "    five_best_individual_list = []\n",
    "    \n",
    "    for i in range(run_times):\n",
    "        best_feasible_individual,logbook,stats,timeSpent = run_GA_framework(ds,evaluateFunction, max_gen,randSeed[i])\n",
    "        # assign best chromosome in order for applying the model: e.g. Navie Bayes \n",
    "        five_best_individual_list.append(best_feasible_individual)\n",
    "        # assign the mean and std\n",
    "        # five_computional_time_dict[\"mean\"].append( logbook.select(\"mean\")[-1])\n",
    "        # five_computional_time_dict[\"std\"].append( logbook.select(\"std\")[-1])\n",
    "        five_computional_time_list.append(timeSpent)\n",
    "\n",
    "        \n",
    "        \n",
    "        print('-'*80)\n",
    "        print('-'*80)\n",
    "        print(\"Running GA with seed: \", randSeed[i])\n",
    "        print('Best Individual fitness: ', best_feasible_individual.fitness.values[0])\n",
    "        print(\"FOllowing are the statistics for each generation with the seed: \", randSeed[i])\n",
    "        print('-'*80)\n",
    "        logbook.header = \"gen\", \"mean\", \"std\", \"best_fintess_current_gen\",\"best_ind_chromosome\"\n",
    "        print(logbook)\n",
    "        print('-'*80)\n",
    "        print('-'*80)\n",
    "    \n",
    "    # transform the selected features by removing unused features\n",
    "    # then apply the model to the selected features\n",
    "    five_acc_score_list = []\n",
    "    for i in range(len(five_best_individual_list)):\n",
    "        df_selected = ds.getDfWithSelectedFeatures(five_best_individual_list[i])\n",
    "        df_selected_transformed = DatasetPart2.getTransformedDf(df_selected)\n",
    "        acc_score = DatasetPart2.run_model(df_selected_transformed, classifier)\n",
    "        \n",
    "        # x_selected_transformed = df_selected_transformed.iloc[:,:-1]\n",
    "        # y_selected_transformed = df_selected_transformed.iloc[:,-1]\n",
    "        # classifier.fit(x_selected_transformed,y_selected_transformed)\n",
    "\n",
    "        # acc_score = accuracy_score(y_selected_transformed, \n",
    "        #                            classifier.predict(x_selected_transformed))\n",
    "        five_acc_score_list.append(acc_score)\n",
    "        print(f\"Accuracy of the model with seed: {randSeed[i]} is: {acc_score}\" )\n",
    "        print('-'*80)\n",
    "        print('-'*80)\n",
    "    \n",
    "    \n",
    "    # # plot the mean and std, bar plot\n",
    "    # for i in range(run_times):\n",
    "    #     plt.bar(title+\"_\"+str(i+1), five_computional_time_dict[\"mean\"][i], yerr=five_computional_time_dict[\"std\"][i])\n",
    "    #     plt.xlabel(\"Mean and Std for each run with different seeds\")\n",
    "    #     plt.ylabel(\"Time Spent (seconds)\")\n",
    "    #     plt.title(f\"{title} \\nmean and std of 5 computional time \")\n",
    "    # plt.show()\n",
    "    \n",
    "    \n",
    "    # for i in range(run_times):\n",
    "    #     plt.bar(title+\"_\"+str(i+1), five_acc_score_list[i])\n",
    "    #     plt.xlabel(\"Accuracy Score for each run with different seeds\")\n",
    "    #     plt.ylabel(\"Accuracy Score\")\n",
    "    #     plt.title(f\"{title} :\\n5 accuracy scores for each run on selected subsets\")    \n",
    "    # plt.show()\n",
    "    \n",
    "    # # plot generation vs fitness for each run\n",
    "    # for i in range(run_times):\n",
    "    #     plt.plot(logbook.select(\"gen\"), logbook.select(\"best_fintess_current_gen\"), \n",
    "    #              label=f\"Seed: {str(randSeed[i])}\")\n",
    "    #     plt.legend(loc=\"lower right\")\n",
    "    #     # print(best_5_avg_fitness_list[i])\n",
    "    # plt.xlabel(\"Generation\")\n",
    "    # plt.ylabel(\"Fitness\")\n",
    "    # plt.title(f\"dataset: {title}\\n 5 Curves for 5 runs \")\n",
    "    # # for addressing the issue of log scale, which is\n",
    "    # # very large and a very small fitness values in a plot\n",
    "    # needSymlog = lambda y_values : min(y_values) < -1e-3\n",
    "    # if needSymlog(logbook.select(\"best_fintess_current_gen\")):\n",
    "    #     plt.yscale('symlog') \n",
    "    # plt.show()\n",
    "    \n",
    "    # print the mean and std of acc and time spent on 5 runs\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{title}\")\n",
    "    print(f\"Mean of the accuracy score is: {np.mean(five_acc_score_list)} \\\n",
    "        \\n  Std of the accuracy score is: {np.std(five_acc_score_list)}\")\n",
    "    print(f\"Mean of the time spent is: {np.mean(five_computional_time_list)} \\\n",
    "        \\n Std of the time spent is: {np.std(five_computional_time_list)}\")\n",
    "    \n",
    "    five_acc_score_dict = {\"mean\":np.mean(five_acc_score_list), \n",
    "                           \"std\":np.std(five_acc_score_list),\n",
    "                           \"list\":five_acc_score_list\n",
    "                           }\n",
    "    five_computional_time_dict = {\"mean\":np.mean(five_computional_time_list),\n",
    "                                  \"std\":np.std(five_computional_time_list),\n",
    "                                  \"list\":five_computional_time_list\n",
    "                                  }\n",
    "    return five_acc_score_dict, five_computional_time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run filterGA 5 times for sonar and wbcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=0'>1</a>\u001b[0m ds_sonar  \u001b[39m=\u001b[39m Sonar\u001b[39m.\u001b[39mconstructFromFile(\u001b[39m\"\u001b[39m\u001b[39m./sonar/sonar.data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=1'>2</a>\u001b[0m sonar_filterGA_acc_score_dict, sonar_filterGA_computional_dict \u001b[39m=\u001b[39m \\\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=2'>3</a>\u001b[0m     run_5_times_with_different_seeds(ds_sonar,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=3'>4</a>\u001b[0m                                      \u001b[39m\"\u001b[39;49m\u001b[39mSonar for FilterGA\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=4'>5</a>\u001b[0m                                      evaluateFilterGA,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=5'>6</a>\u001b[0m                                      classifier\u001b[39m=\u001b[39;49mCONSTANTS_DICT[\u001b[39m\"\u001b[39;49m\u001b[39mCLASSIFIER\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=6'>7</a>\u001b[0m                                      max_gen\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=7'>8</a>\u001b[0m                                      run_times\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb Cell 14\u001b[0m in \u001b[0;36mrun_5_times_with_different_seeds\u001b[0;34m(ds, title, evaluateFunction, classifier, max_gen, randSeed, run_times)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=14'>15</a>\u001b[0m five_best_individual_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(run_times):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=17'>18</a>\u001b[0m     best_feasible_individual,logbook,stats,timeSpent \u001b[39m=\u001b[39m run_GA_framework(ds,evaluateFunction, max_gen,randSeed[i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=18'>19</a>\u001b[0m     \u001b[39m# assign best chromosome in order for applying the model: e.g. Navie Bayes \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=19'>20</a>\u001b[0m     five_best_individual_list\u001b[39m.\u001b[39mappend(best_feasible_individual)\n",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb Cell 14\u001b[0m in \u001b[0;36mrun_GA_framework\u001b[0;34m(ds, evaluateFunction, max_gen, randSeed)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=34'>35</a>\u001b[0m \u001b[39m# start the evolution\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m gen_counter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_gen):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=36'>37</a>\u001b[0m     \u001b[39m# assign the fitness to each individual in the current generation \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=37'>38</a>\u001b[0m     evaluate_fitness_values(population)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=38'>39</a>\u001b[0m     \u001b[39m# for recording the time spent on each generation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=39'>40</a>\u001b[0m     genStartTime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb Cell 14\u001b[0m in \u001b[0;36mrun_GA_framework.<locals>.evaluate_fitness_values\u001b[0;34m(pop)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_fitness_values\u001b[39m(pop) :\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=25'>26</a>\u001b[0m     \u001b[39m\"\"\"Update the fitness values of each individual for the given the population\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=26'>27</a>\u001b[0m     fitnesses \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(toolbox\u001b[39m.\u001b[39;49mevaluate, pop))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=27'>28</a>\u001b[0m     \u001b[39mfor\u001b[39;00m ind, fit \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(pop, fitnesses):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=28'>29</a>\u001b[0m         ind\u001b[39m.\u001b[39mfitness\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m fit\n",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb Cell 14\u001b[0m in \u001b[0;36mevaluateFilterGA\u001b[0;34m(ds, individual)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=47'>48</a>\u001b[0m \u001b[39m\"\"\"Goodness of a individual(i.e. feature subset) independent of the classifier.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=48'>49</a>\u001b[0m \u001b[39mInformation Gain is used to evaluate the goodness of a feature subset.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=49'>50</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=52'>53</a>\u001b[0m \u001b[39m    individual (creator.Individual): _description_\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=53'>54</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=54'>55</a>\u001b[0m \u001b[39m# get the df with selected features, StandardScaler is already used to scale continous features into the discrete values\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=55'>56</a>\u001b[0m df_selected \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mgetDfWithSelectedFeatures(individual)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=56'>57</a>\u001b[0m df_selected_transformed \u001b[39m=\u001b[39m DatasetPart2\u001b[39m.\u001b[39mgetTransformedDf(df_selected)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=57'>58</a>\u001b[0m y \u001b[39m=\u001b[39m df_selected\u001b[39m.\u001b[39miloc[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb Cell 14\u001b[0m in \u001b[0;36mDatasetPart2.getDfWithSelectedFeatures\u001b[0;34m(self, selectedFeatures)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=54'>55</a>\u001b[0m     isSelected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m selectedFeatures[i] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=55'>56</a>\u001b[0m     \u001b[39mif\u001b[39;00m isSelected:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=56'>57</a>\u001b[0m         \u001b[39m# concat this feature to the returned dataframe\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=57'>58</a>\u001b[0m         returnedDf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([returnedDf,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf\u001b[39m.\u001b[39;49miloc[:,i]],axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=58'>59</a>\u001b[0m \u001b[39m# concat the class column\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zhouyun/Desktop/aiml426-a1/p2/p2.ipynb#ch0000013?line=59'>60</a>\u001b[0m returnedDf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([returnedDf,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39miloc[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/pandas/core/reshape/concat.py:294\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m     92\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    102\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FrameOrSeriesUnion:\n\u001b[1;32m    103\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    295\u001b[0m         objs,\n\u001b[1;32m    296\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    297\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    298\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    299\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    300\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    301\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    302\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    303\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    304\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/pandas/core/reshape/concat.py:479\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_integrity \u001b[39m=\u001b[39m verify_integrity\n\u001b[1;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[0;32m--> 479\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_axes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_new_axes()\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/pandas/core/reshape/concat.py:549\u001b[0m, in \u001b[0;36m_Concatenator._get_new_axes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_new_axes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[1;32m    548\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_result_dim()\n\u001b[0;32m--> 549\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    550\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concat_axis \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_comb_axis(i)\n\u001b[1;32m    551\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)\n\u001b[1;32m    552\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/pandas/core/reshape/concat.py:550\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_new_axes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[1;32m    548\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_result_dim()\n\u001b[1;32m    549\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m--> 550\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concat_axis \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_comb_axis(i)\n\u001b[1;32m    551\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)\n\u001b[1;32m    552\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/pandas/_libs/properties.pyx:37\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/pandas/core/reshape/concat.py:605\u001b[0m, in \u001b[0;36m_Concatenator._get_concat_axis\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[39mreturn\u001b[39;00m idx\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     concat_axis \u001b[39m=\u001b[39m _concat_indexes(indexes)\n\u001b[1;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m     concat_axis \u001b[39m=\u001b[39m _make_concat_multiindex(\n\u001b[1;32m    608\u001b[0m         indexes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames\n\u001b[1;32m    609\u001b[0m     )\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/pandas/core/reshape/concat.py:623\u001b[0m, in \u001b[0;36m_concat_indexes\u001b[0;34m(indexes)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_concat_indexes\u001b[39m(indexes) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Index:\n\u001b[0;32m--> 623\u001b[0m     \u001b[39mreturn\u001b[39;00m indexes[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mappend(indexes[\u001b[39m1\u001b[39;49m:])\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/pandas/core/indexes/base.py:4680\u001b[0m, in \u001b[0;36mIndex.append\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4677\u001b[0m names \u001b[39m=\u001b[39m {obj\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m to_concat}\n\u001b[1;32m   4678\u001b[0m name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(names) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\n\u001b[0;32m-> 4680\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_concat(to_concat, name)\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/pandas/core/indexes/base.py:4689\u001b[0m, in \u001b[0;36mIndex._concat\u001b[0;34m(self, to_concat, name)\u001b[0m\n\u001b[1;32m   4686\u001b[0m to_concat_vals \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39m_values \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m to_concat]\n\u001b[1;32m   4688\u001b[0m result \u001b[39m=\u001b[39m concat_compat(to_concat_vals)\n\u001b[0;32m-> 4689\u001b[0m \u001b[39mreturn\u001b[39;00m Index(result, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/pandas/core/indexes/base.py:462\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m arr \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39masarray_tuplesafe(data, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    461\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m     arr \u001b[39m=\u001b[39m _maybe_cast_data_without_dtype(arr)\n\u001b[1;32m    463\u001b[0m     dtype \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    465\u001b[0m     \u001b[39mif\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/pandas/core/indexes/base.py:6417\u001b[0m, in \u001b[0;36m_maybe_cast_data_without_dtype\u001b[0;34m(subarr)\u001b[0m\n\u001b[1;32m   6397\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_cast_data_without_dtype\u001b[39m(subarr: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[1;32m   6398\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   6399\u001b[0m \u001b[39m    If we have an arraylike input but no passed dtype, try to infer\u001b[39;00m\n\u001b[1;32m   6400\u001b[0m \u001b[39m    a supported dtype.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6408\u001b[0m \u001b[39m    np.ndarray or ExtensionArray\u001b[39;00m\n\u001b[1;32m   6409\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   6411\u001b[0m     result \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmaybe_convert_objects(\n\u001b[1;32m   6412\u001b[0m         subarr,\n\u001b[1;32m   6413\u001b[0m         convert_datetime\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   6414\u001b[0m         convert_timedelta\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   6415\u001b[0m         convert_period\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   6416\u001b[0m         convert_interval\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m-> 6417\u001b[0m         dtype_if_all_nat\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mdtype(\u001b[39m\"\u001b[39;49m\u001b[39mdatetime64[ns]\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   6418\u001b[0m     )\n\u001b[1;32m   6419\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mc\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m   6420\u001b[0m         \u001b[39mreturn\u001b[39;00m subarr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds_sonar  = Sonar.constructFromFile(\"./sonar/sonar.data\")\n",
    "sonar_filterGA_acc_score_dict, sonar_filterGA_computional_dict = \\\n",
    "    run_5_times_with_different_seeds(ds_sonar,\n",
    "                                     \"Sonar for FilterGA\",\n",
    "                                     evaluateFilterGA,\n",
    "                                     classifier=CONSTANTS_DICT[\"CLASSIFIER\"],\n",
    "                                     max_gen=10,\n",
    "                                     run_times=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"acc_score for sonar filter GA: \\n\\t{sonar_filterGA_acc_score_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Time spent for sonar filter GA: \\n\\t{sonar_filterGA_computional_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_wbcd = Wbcd.constructFromFile(\"./wbcd/wbcd.data\")\n",
    "wbcd_filterGA_acc_score_dict, wbcd_filterGA_computional_dict = \\\n",
    "    run_5_times_with_different_seeds(ds_wbcd,\n",
    "                                     \"wbcd for FilterGA\",\n",
    "                                     evaluateFilterGA,\n",
    "                                     classifier=CONSTANTS_DICT[\"CLASSIFIER\"],\n",
    "                                     max_gen=100,\n",
    "                                     run_times=5\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run wrapperGA for 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/information-gain-and-mutual-information/\n",
    "ds_sonar  = Sonar.constructFromFile(\"./sonar/sonar.data\")\n",
    "sonar_WrapperGA_acc_score_dict, sonar_WrapperGA_computional_dict = \\\n",
    "    run_5_times_with_different_seeds(ds_sonar,\n",
    "                                     \"Sonar for WrapperGA\",\n",
    "                                     evaluateWrapperGA,\n",
    "                                     classifier=CONSTANTS_DICT[\"CLASSIFIER\"],\n",
    "                                     max_gen=100,\n",
    "                                     run_times=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_wbcd = Wbcd.constructFromFile(\"./wbcd/wbcd.data\")\n",
    "wbcd_WrapperGA_acc_score_dict, wbcd_WrapperGA_computional_dict = \\\n",
    "    run_5_times_with_different_seeds(ds_wbcd,\n",
    "                                     \"wbcd for WrapperGA\",\n",
    "                                     evaluateWrapperGA,\n",
    "                                     classifier=CONSTANTS_DICT[\"CLASSIFIER\"],\n",
    "                                     max_gen=100,\n",
    "                                     run_times=5\n",
    "                                     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52da647447b8fe2208076266408c42f82750713fb5b92055dee0a0742687bf52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
